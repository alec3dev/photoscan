<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0">
  <title>Upright Face Capture</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.21.0"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface"></script>
  <script src="https://cdn.jsdelivr.net/npm/exif-js"></script>
  <style>
    video {
      width: 100%;
      max-width: 600px;
    }
    .status {
      padding: 5px;
      margin-top: 10px;
    }
    .status.scanning {color: orange;}
    .status.success {color: green;}
    .status.error {color: red;}
    .face-card {
      display: inline-block;
      margin: 10px;
      padding: 5px;
      border: 1px solid #ccc;
      text-align: center;
    }
    .face-image {
      width: 150px;
      height: 150px;
      object-fit: cover;
      display: block;
      margin-bottom: 5px;
    }
    .face-name {
      display: block;
      width: 140px;
      margin: 3px auto;
    }
  </style>
</head>
<body>
  <h2>Upright Face Capture</h2>
  <video id="video" autoplay playsinline></video><br>
  <button id="startCamera">Start Camera</button>
  <button id="switchCamera" disabled>Switch Camera</button>
  <button id="capture" disabled>Capture Faces</button>
  <input type="file" id="fileInput">
  <div id="status" class="status"></div>
  <div id="facesContainer"></div>

  <script>
    const video = document.getElementById('video');
    const startCameraBtn = document.getElementById('startCamera');
    const captureBtn = document.getElementById('capture');
    const switchCameraBtn = document.getElementById('switchCamera');
    const fileInput = document.getElementById('fileInput');
    const statusDiv = document.getElementById('status');
    const facesContainer = document.getElementById('facesContainer');

    let stream = null;
    let currentFacingMode = 'user';
    let model = null;

    async function loadFaceDetectionModel() {
      statusDiv.textContent = "Loading face detection model...";
      statusDiv.className = "status scanning";
      try {
        model = await blazeface.load();
        statusDiv.textContent = "Model loaded successfully!";
        statusDiv.className = "status success";
      } catch (err) {
        statusDiv.textContent = "Failed to load face detection model.";
        statusDiv.className = "status error";
        console.error(err);
      }
    }

    async function startCamera() {
      statusDiv.textContent = "Accessing camera...";
      statusDiv.className = "status scanning";
      try {
        const constraints = {
          video: {
            facingMode: currentFacingMode,
            width: {ideal: 1280},
            height: {ideal: 720}
          }
        };
        stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;
        startCameraBtn.disabled = true;
        captureBtn.disabled = false;
        switchCameraBtn.disabled = false;
        statusDiv.textContent = "Camera active. Position faces then click Capture.";
        statusDiv.className = "status success";
      } catch (err) {
        statusDiv.textContent = "Error accessing camera: " + err.message;
        statusDiv.className = "status error";
        console.error(err);
      }
    }

    async function switchCamera() {
      if (stream) {
        stream.getTracks().forEach(t => t.stop());
      }
      currentFacingMode = currentFacingMode === 'environment' ? 'user' : 'environment';
      await startCamera();
    }

    // read EXIF and rotate an Image() upright
    function rotateImageByOrientation(img, orientation) {
      const canvas = document.createElement('canvas');
      const ctx = canvas.getContext('2d');
      let width = img.width, height = img.height;
      let rotate = 0;

      if (orientation === 3) rotate = 180;
      if (orientation === 6) rotate = 90;
      if (orientation === 8) rotate = 270;

      if (rotate === 90 || rotate === 270) {
        canvas.width = height;
        canvas.height = width;
      } else {
        canvas.width = width;
        canvas.height = height;
      }

      ctx.translate(canvas.width/2, canvas.height/2);
      ctx.rotate(rotate * Math.PI / 180);
      ctx.drawImage(img, -width/2, -height/2);

      const newImg = new Image();
      newImg.src = canvas.toDataURL('image/jpeg');
      return newImg;
    }

    fileInput.addEventListener('change', (e) => {
      const file = e.target.files[0];
      const img = new Image();
      img.onload = () => {
        EXIF.getData(img, function() {
          const orientation = EXIF.getTag(this, 'Orientation') || 1;
          const rotated = rotateImageByOrientation(img, orientation);
          rotated.onload = () => detectFacesOnImage(rotated);
        });
      };
      img.src = URL.createObjectURL(file);
    });

    async function detectFacesOnImage(img) {
      const canvas = document.createElement('canvas');
      canvas.width = img.width;
      canvas.height = img.height;
      const ctx = canvas.getContext('2d');
      ctx.drawImage(img, 0, 0);
      const predictions = await model.estimateFaces(canvas, false);
      facesContainer.innerHTML = '';
      if (predictions.length > 0) {
        for (let i = 0; i < predictions.length; i++) {
          await extractAndCorrectFace(canvas, predictions[i], i);
        }
        statusDiv.textContent = `Processed ${predictions.length} face(s).`;
        statusDiv.className = "status success";
      } else {
        statusDiv.textContent = "No faces detected. Try again.";
        statusDiv.className = "status error";
      }
    }

    async function captureImage() {
      statusDiv.textContent = "Capturing image and detecting faces...";
      statusDiv.className = "status scanning";

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      const ctx = canvas.getContext('2d');

      // Mirror for front camera
      ctx.save();
      ctx.scale(-1, 1);
      ctx.drawImage(video, -canvas.width, 0, canvas.width, canvas.height);
      ctx.restore();

      await detectFacesOnImage(canvas);
    }

    async function extractAndCorrectFace(sourceCanvas, face, index) {
      return new Promise((resolve) => {
        const start = face.topLeft;
        const end = face.bottomRight;
        const width = end[0] - start[0];
        const height = end[1] - start[1];
        const padding = Math.min(width, height) * 0.2;
        const size = Math.max(width, height) + padding * 2;

        // rotation angle from eyes
        let rotationAngle = 0;
        if (face.landmarks && face.landmarks.length >= 2) {
          const leftEye = face.landmarks[0];
          const rightEye = face.landmarks[1];
          const dy = rightEye[1] - leftEye[1];
          const dx = rightEye[0] - leftEye[0];
          rotationAngle = Math.atan2(dy, dx); // radians
        }

        // rotate & draw
        const faceCanvas = document.createElement('canvas');
        faceCanvas.width = size;
        faceCanvas.height = size;
        const faceCtx = faceCanvas.getContext('2d');

        faceCtx.clearRect(0, 0, size, size);
        faceCtx.translate(size / 2, size / 2);
        faceCtx.rotate(-rotationAngle);
        faceCtx.translate(-size / 2, -size / 2);

        faceCtx.drawImage(
          sourceCanvas,
          start[0] - padding,
          start[1] - padding,
          width + padding * 2,
          height + padding * 2,
          0, 0, size, size
        );

        const faceDataUrl = faceCanvas.toDataURL('image/jpeg');

        const faceCard = document.createElement('div');
        faceCard.className = 'face-card';

        const faceImg = document.createElement('img');
        faceImg.className = 'face-image';
        faceImg.src = faceDataUrl;
        faceImg.alt = `Face ${index + 1}`;

        const nameInput = document.createElement('input');
        nameInput.type = 'text';
        nameInput.className = 'face-name';
        nameInput.value = `person_${index + 1}`;

        const downloadBtn = document.createElement('button');
        downloadBtn.textContent = 'Download';
        downloadBtn.onclick = () => {
          const link = document.createElement('a');
          link.download = `${nameInput.value || `person_${Date.now()}_${index+1}`}.jpg`;
          link.href = faceDataUrl;
          link.click();
        };

        faceCard.appendChild(faceImg);
        faceCard.appendChild(nameInput);
        faceCard.appendChild(downloadBtn);
        facesContainer.appendChild(faceCard);

        resolve();
      });
    }

    startCameraBtn.addEventListener('click', startCamera);
    switchCameraBtn.addEventListener('click', switchCamera);
    captureBtn.addEventListener('click', captureImage);

    loadFaceDetectionModel();
  </script>
</body>
</html>
